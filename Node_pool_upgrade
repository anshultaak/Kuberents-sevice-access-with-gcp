NODE-POOL-UPGRADE

Step-1: 
Existing node pools are having Secure boot "Disabled". So we need to create new node pool with Secure boot "Enabled".

Note: Use same configuartion as old node-pool in new node-pool.

Step-2:
After you create a new node pool, your workloads are still running on old node-pool. Kubernetes does not reschedule Pods as long as they are running and available.

## To migrate these Pods to the new node pool, you must do the following:

1. Cordon the existing node pool: This operation marks the nodes in the existing node pool (old node-pool) as unschedulable. Kubernetes stops scheduling new Pods to these nodes once you mark them as unschedulable.

kubectl get nodes -l cloud.google.com/gke-nodepool=<old_node-pool_name>

2. The following shell command iterates each node in old node-pool, marks them unschedulable, and drains them by evicting Pods with an allotted graceful termination period of 10 seconds:

for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=pool-3 -o=name); do
  kubectl drain --force --ignore-daemonsets --delete-emptydir-data --grace-period=10 "$node";
done

3. Once this command completes, you should see that the old node-pool nodes have SchedulingDisabled status in the node list:

kubectl get nodes

Additionally, you should see that the Pods are now running on the new node-pool nodes:

kubectl get pods --all-namespaces -o=wide


## Deleting the old node pool: 

1. Once Kubernetes reschedules all Pods to the new node-pool, it is now safe to delete the old node-pool as it is no longer necessary. Run the following command to delete the old node-pool:

gcloud container node-pools delete pool-3 --cluster e4gt00appkub100

2. Once this operation completes, you should have a single node pool for your container cluster, which is the new node-pool:

gcloud container node-pools list --cluster <Cluster_name>
